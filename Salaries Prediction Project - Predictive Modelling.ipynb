{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling\n",
    "This notebook goes through the process of setting up predictive modelling in order to predict salaries for a job opening. This file contains the following steps:\n",
    "\n",
    "1. Preprocessing Data\n",
    "2. Training different models\n",
    "3. Finding the best model with lowest MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we cleaned the train csv file and saved as 'train_df_clean.csv'. This file is already consolidated over jobkey_id index. We will also load this file along with the 'test_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df = pd.read_csv('train_df_clean.csv')\n",
    "clean_test_df = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling Dataframe\n",
    "Shuffling Dataframe may help to improve cross-validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df = shuffle(clean_train_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "numeric_vars = ['yearsExperience', 'milesFromMetropolis']\n",
    "target_var = 'salary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = clean_train_df[target_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_feature_df(df, cat_vars=None, num_vars=None):\n",
    "    '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = one_hot_encode_feature_df(clean_train_df, cat_vars=categorical_vars, num_vars=numeric_vars)\n",
    "test_df = one_hot_encode_feature_df(clean_test_df, cat_vars=categorical_vars, num_vars=numeric_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and Training Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Initializing model lists and dict\n",
    "2. create models + Hyperparameter tuning\n",
    "3. Model summary\n",
    "\n",
    "Models:\n",
    "1. Linear Regression\n",
    "2. RandomForestRegressor\n",
    "3. GradientBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing model lists and dict\n",
    "models = []\n",
    "mean_mse = {}\n",
    "cv_std = {}\n",
    "res = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of processes to run in parallel\n",
    "num_procs = 2\n",
    "\n",
    "#shared model paramaters\n",
    "verbose_lvl = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiazlizing Models\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(n_estimators=150, n_jobs=num_procs, max_depth=25, min_samples_split=60, max_features=30, verbose=verbose_lvl)\n",
    "gb = GradientBoostingRegressor(n_estimators=150, max_depth=5, loss='ls', verbose=verbose_lvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.extend([lr, rf, gb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std):\n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv=2, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "    mean_mse[model] = -1.0*np.mean(neg_mse)\n",
    "    cv_std[model] = np.std(neg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      " LinearRegression()\n",
      "Average MSE:\n",
      " 384.47499047251506\n",
      "Standard deviation during CV:\n",
      " 3.3891256464357866e-05\n",
      "Standard deviation during CV:\n",
      " 3.3891256464357866e-05\n",
      "\n",
      "Model:\n",
      " RandomForestRegressor(max_depth=25, max_features=30, min_samples_split=60,\n",
      "                      n_estimators=150, n_jobs=2, verbose=5)\n",
      "Average MSE:\n",
      " 367.5588277034447\n",
      "Standard deviation during CV:\n",
      " 0.04838140050742368\n",
      "Standard deviation during CV:\n",
      " 0.04838140050742368\n",
      "\n",
      "Model:\n",
      " GradientBoostingRegressor(max_depth=5, n_estimators=150, verbose=5)\n",
      "Average MSE:\n",
      " 357.33422543210236\n",
      "Standard deviation during CV:\n",
      " 0.09232045687170398\n",
      "Standard deviation during CV:\n",
      " 0.09232045687170398\n"
     ]
    }
   ],
   "source": [
    "#cross validate models and print summaries to compare each model\n",
    "for model in models:\n",
    "    train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std)\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average MSE:\\n', mean_mse[model])\n",
    "    print('Standard deviation during CV:\\n', cv_std[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Average MSE | SD during CV |\n",
    "| ---- | ---- | ---- |\n",
    "| Linear Regression | 384.475 | 3.389 |\n",
    "| Random Forest | 367.559 | 0.048 |\n",
    "| Gradient Boost | 357.334 | 0.092 |\n",
    "\n",
    "We saw the lowest Average Mean Squared Error with Gradient Boost Regressor Model making it the best model among the three used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
